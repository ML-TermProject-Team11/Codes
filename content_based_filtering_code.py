# -*- coding: utf-8 -*-
"""Content_based_filtering_CODE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19fuOJXv2o2KbDD3IPjSb8E83-JbGfcJt
"""

!pip -q install pyarrow fastparquet pandas scipy scikit-learn

CONTENT_PATH = "movielens_content.parquet"          # columns: movieId,title,genres,[tmdbId,imdbId,genre_list]
TMDB_PATH    = "tmdb_items_enriched_update.parquet"        # columns: movieId,director,genres_tmdb,[runtime,popularity,release_year,title_tmdb,original_language]
GENOME_PATH  = "movielens_genome_long.parquet"      # columns: movieId,tagId,relevance,[tag]
RATINGS_PATH = "netflix_ratings.parquet"  # columns: movieId,customerId,rating,date

import pandas as pd
import numpy as np
import ast
from typing import List, Dict, Tuple, Optional
from scipy import sparse
from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder, normalize
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.metrics.pairwise import cosine_similarity

# data parsing
def parse_multi_value_cell(x: Optional[str]) -> List[str]:
    """
    ë¬¸ìì—´ë¡œ ëœ ë‹¤ì¤‘ ê°’ ì…€ -> ë¦¬ìŠ¤íŠ¸ë¡œ íŒŒì‹±.
    - í—ˆìš© í¬ë§·: 'A|B', 'A, B', 'A;B', '["A","B"]'
    - ê³µë°±/NaN/ë¹ˆ ë¬¸ìì—´ -> ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    """
    if x is None or (isinstance(x, float) and np.isnan(x)) or (isinstance(x, str) and x.strip() == ""):
        return []
    s = str(x).strip()
    # JSON/Python ë¦¬ìŠ¤íŠ¸ í¬ë§· ìš°ì„ 
    if s.startswith("[") and s.endswith("]"):
        try:
            parsed = ast.literal_eval(s)
            return [str(v).strip() for v in parsed if str(v).strip()]
        except Exception:
            pass
    # ì¼ë°˜ êµ¬ë¶„ì
    for sep in ["|", ",", ";", "/"]:
        if sep in s:
            return [t.strip() for t in s.split(sep) if t.strip()]
    # êµ¬ë¶„ì ì—†ìœ¼ë©´ ë‹¨ì¼ ê°’
    return [s]

def coalesce_genre_list(genres_raw: Optional[str], genre_list_raw: Optional[str]) -> List[str]:
    """
    genre_listê°€ ìˆìœ¼ë©´ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©, ì—†ìœ¼ë©´ genres íŒŒì‹±
    """
    g2 = parse_multi_value_cell(genre_list_raw)
    if g2:
        return g2
    return parse_multi_value_cell(genres_raw)

def normalize_genre_tokens(genres: List[str]) -> List[str]:
    """
    ì‚¬ì „ ê¸°ë°˜ ì¥ë¥´ í† í° í‘œì¤€í™”(ëŒ€ì†Œë¬¸ì/ë™ì˜ì–´ ì •ë¦¬).
    """
    mapping = {
        "sci-fi": "Science Fiction",
        "science fiction": "Science Fiction",
        "scifi": "Science Fiction",
        "romcom": "Romance",
        "children": "Family",
    }
    out = []
    for g in genres:
        key = g.strip().lower()
        out.append(mapping.get(key, g.strip()))
    return out

def normalize_people_tokens(names: List[str]) -> List[str]:
    """
    ê°ë… ì´ë¦„ í‘œì¤€í™”(ëŒ€ì†Œë¬¸ì/ê³µë°± ì •ë¦¬).
    """
    return [n.strip() for n in names]

# data file loading
def load_movielens_content(path: str) -> pd.DataFrame:
    """
    input columns:
      - í•„ìˆ˜: movieId, title, genres
      - ë³´ì¡°: tmdbId, imdbId, genre_list
    final output column:
      - genres_parsed (ë¦¬ìŠ¤íŠ¸)
    """
    df = pd.read_parquet(path)
    for c in ["tmdbId", "imdbId", "genre_list"]:
        if c not in df.columns:
            df[c] = np.nan
    df["genres_parsed"] = [coalesce_genre_list(g, gl) for g, gl in zip(df["genres"], df.get("genre_list"))]
    df["genres_parsed"] = df["genres_parsed"].apply(normalize_genre_tokens)
    keep = ["movieId", "title", "tmdbId", "imdbId", "genres_parsed"]
    missing = set(["movieId","title","genres"]) - set(df.columns)
    if missing:
        raise ValueError(f"[movielens_content] missing columns: {missing}")
    return df[keep]

def load_tmdb_items(path: str) -> pd.DataFrame:
    """
    input columns:
      - í•„ìˆ˜: movieId, director, genres_tmdb
      - ë³´ì¡°: runtime, popularity, release_year, title_tmdb, original_language
    final output column:
      - director_list, genres_tmdb_list (ë¦¬ìŠ¤íŠ¸)
    """
    df = pd.read_parquet(path)
    if "director" not in df.columns:
        df["director"] = ""
    if "genres_tmdb" not in df.columns:
        df["genres_tmdb"] = ""
    df["director_list"] = df["director"].apply(parse_multi_value_cell).apply(normalize_people_tokens)
    df["genres_tmdb_list"] = df["genres_tmdb"].apply(parse_multi_value_cell).apply(normalize_genre_tokens)
    keep = ["movieId", "director_list", "genres_tmdb_list", "runtime", "popularity",
            "release_year", "title_tmdb", "original_language"]
    for c in keep:
        if c not in df.columns:
            df[c] = np.nan
    if "movieId" not in df.columns:
        raise ValueError("[tmdb_items_enriched] missing column: movieId")
    return df[keep]

def load_genome_long(path: str) -> pd.DataFrame:
    """
    input columns:
      - í•„ìˆ˜: movieId, tagId, relevance
      - ë³´ì¡°: tag (ì—†ìœ¼ë©´ tagId ë¬¸ìì—´ë¡œ ëŒ€ì²´)
    final output column:
      - (movieId, tagId, relevance, tag)
    """
    df = pd.read_parquet(path)
    if "movieId" not in df.columns or "tagId" not in df.columns or "relevance" not in df.columns:
        raise ValueError("[genome_long] must include columns: movieId, tagId, relevance")
    if "tag" not in df.columns:
        df["tag"] = df["tagId"].astype(str)
    return df[["movieId", "tagId", "relevance", "tag"]]

from sklearn.preprocessing import normalize as sk_normalize

# calculate content(movie) feature(Genre/Director/Tags) vector
def build_item_feature_matrices(content_df: pd.DataFrame, tmdb_df: pd.DataFrame, genome_df: pd.DataFrame,
                                weights=(1.0, 1.8, 1.0), topk_tags_per_movie=25):
    # ì¥ë¥´
    merged = content_df[["movieId", "genres_parsed"]].merge(
        tmdb_df[["movieId", "genres_tmdb_list"]], on="movieId", how="left"
    )
    merged["genres_tmdb_list"] = merged["genres_tmdb_list"].apply(lambda x: x if isinstance(x, list) else [])
    merged["genres_parsed"]    = merged["genres_parsed"].apply(lambda x: x if isinstance(x, list) else [])
    merged["all_genres"] = [list(set(a)|set(b)) for a,b in zip(merged["genres_parsed"], merged["genres_tmdb_list"])]
    mlb_genre = MultiLabelBinarizer(sparse_output=True)
    X_genre = mlb_genre.fit_transform(merged["all_genres"]).tocsr()
    genre_vocab = list(mlb_genre.classes_)
    movie_index = merged["movieId"].reset_index(drop=True)

    # ê°ë…
    df_dir = tmdb_df.set_index("movieId").reindex(movie_index).reset_index()
    directors = df_dir["director_list"].apply(lambda x: x if isinstance(x, list) else [])
    mlb_dir = MultiLabelBinarizer(sparse_output=True)
    X_dir = mlb_dir.fit_transform(directors).tocsr()
    dir_vocab = list(mlb_dir.classes_)

    # Genome (TF-IDF)
    g = genome_df.copy()
    g = g[g["movieId"].isin(set(movie_index))]
    g = g.sort_values(["movieId","relevance"], ascending=[True, False]).groupby("movieId").head(topk_tags_per_movie)
    tag_text = g["tag"].astype(str).fillna(g["tagId"].astype(str))
    le = LabelEncoder()
    tag_idx = le.fit_transform(tag_text)
    pos_map = {mid: i for i, mid in enumerate(movie_index)}
    rows = g["movieId"].map(pos_map).values
    cols = tag_idx
    data = g["relevance"].values
    M = sparse.coo_matrix((data, (rows, cols)), shape=(len(movie_index), len(le.classes_))).tocsr()
    tfidf = TfidfTransformer(norm="l2", use_idf=True, smooth_idf=True)
    X_genome = tfidf.fit_transform(M)

    # weight sum for feature
    """
    - ê° ë¸”ë¡ì„ L2 ì •ê·œí™” í›„(ìŠ¤ì¼€ì¼ ì°¨ì´ ì œê±°) íŠ¹ì§• ë³„ ê°€ì¤‘ì¹˜ ê³±
    - ì¢Œìš°ë¡œ hstack í•˜ì—¬ í•˜ë‚˜ì˜ ì•„ì´í…œ ë²¡í„°ë¡œ í†µí•©
    """
    Xg = sk_normalize(X_genre, norm="l2") * weights[0] if X_genre.shape[1] else X_genre
    Xd = sk_normalize(X_dir,   norm="l2") * weights[1] if X_dir.shape[1]   else X_dir
    Xt = X_genome * weights[2]  # TF-IDF + L2
    X_items = sparse.hstack([Xg, Xd, Xt]).tocsr()

    # ìµœì¢… ê²°í•© í›„ ì „ì—­ L2 ì •ê·œí™” (ì½”ì‚¬ì¸ ì ê³±ì„ ì§„ì§œ ì½”ì‚¬ì¸ìœ¼ë¡œ)
    X_items = sk_normalize(X_items, norm="l2", copy=False)

    vocab = {"genre": genre_vocab, "director": dir_vocab, "genome": list(le.classes_)}
    return X_items, movie_index, vocab

"""Predict with test dataset"""

import pyarrow as pa
import pyarrow.dataset as ds
import pyarrow.compute as pc
import pandas as pd

meta_movie_ids = pd.read_parquet(CONTENT_PATH)["movieId"].astype("int64").tolist()
movie_ids_arr = pa.array(meta_movie_ids, type=pa.int64())

# 2) Netflix dataset ìŠ¤ìº”
dataset = ds.dataset(RATINGS_PATH, format="parquet")
cols = ["movieId", "customerId", "rating", "date"]

# 3) ë©”íƒ€ì— ì¡´ì¬í•˜ëŠ” movieIdë§Œ ë‚¨ê¸°ëŠ” í•„í„°
filter_expr = pc.is_in(ds.field("movieId"), value_set=movie_ids_arr)

# 4) í•„í„°/ì»¬ëŸ¼ì„ pushdowní•˜ì—¬ ë°”ë¡œ í…Œì´ë¸”ë¡œ ì ì¬
tbl = dataset.to_table(columns=cols, filter=filter_expr)

# 5) Pandas ë³€í™˜ í›„ "ìœ ì € í•´ì‹œ ìƒ˜í”Œë§" ì§„í–‰ (ì—¬ê¸°ì„œ ìƒ˜í”Œë§)
df_small = tbl.to_pandas()
mod_base = 100   # 1% ìƒ˜í”Œ
mask = (df_small["customerId"].astype("int64") % mod_base) == 0
df_small = df_small.loc[mask].copy()

# 6) ë‚ ì§œ íŒŒì‹±/ì •ë¦¬
df_small["timestamp"] = pd.to_datetime(df_small["date"], errors="coerce")
df_small = df_small.dropna(subset=["timestamp"])
df_small = df_small.rename(columns={"customerId":"userId"})
df_small = df_small[["userId","movieId","rating","timestamp"]].astype({"userId":"int64","movieId":"int64"})

# 7) ì¶•ì†Œ parquet ì €ì¥
SAMPLED_RATINGS_PATH = "netflix_ratings_sample.parquet"
df_small.to_parquet(SAMPLED_RATINGS_PATH, index=False)
print("sampled interactions saved:", len(df_small), SAMPLED_RATINGS_PATH)

# load test dataset
def load_netflix_interactions_parquet(path: str) -> pd.DataFrame:
    df = pd.read_parquet(path)
    need = {"movieId","customerId","rating","date"}
    missing = need - set(df.columns)
    if missing:
        raise ValueError(f"[netflix_ratings] missing columns: {missing}")
    # date â†’ datetime parsing + sort
    d = df.copy()
    d["timestamp"] = pd.to_datetime(d["date"], errors="coerce")
    d = d.dropna(subset=["timestamp"])
    return d[["customerId","movieId","rating","timestamp"]].rename(columns={"customerId":"userId"})

# Leave-Last-k-Out
def temporal_leave_last_k_out(df: pd.DataFrame, k: int = 1, pos_threshold: float = 4.0):
    """
    ìœ ì €ë³„ timestamp ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ -> ë§ˆì§€ë§‰(ìµœì‹ ) kê°œë¥¼ Test, ë‚˜ë¨¸ì§€ Train.
    positive rate: rating >= pos_threshold
    """
    df_sorted = df.sort_values(["userId","timestamp","movieId"])
    train_pos, test_pos, seen = {}, {}, {}
    for uid, g in df_sorted.groupby("userId"):
        movies = g["movieId"].tolist()
        ratings = g["rating"].tolist()
        if len(movies) <= k:
            continue
        train_movies = movies[:-k]
        train_ratings = ratings[:-k]
        test_movies  = movies[-k:]
        test_ratings = ratings[-k:]
        train_pos_set = {m for m, r in zip(train_movies, train_ratings) if r >= pos_threshold}
        test_pos_set  = {m for m, r in zip(test_movies,  test_ratings)  if r >= pos_threshold}
        if len(test_pos_set) == 0:
            # í…ŒìŠ¤íŠ¸ì— ì–‘ì„± rate ì—†ìœ¼ë©´ ì •ë‹µì´ ì—†ìœ¼ë‹ˆ í‰ê°€ ì œì™¸
            continue
        train_pos[uid] = train_pos_set
        test_pos[uid]  = test_pos_set
        seen[uid]      = set(train_movies)
    return train_pos, test_pos, seen

def load_interactions_auto(path: str) -> pd.DataFrame:
    """
    ë‹¤ìŒ ìŠ¤í‚¤ë§ˆë¥¼ ìë™ìœ¼ë¡œ ê°ì§€í•´ í‘œì¤€ ìŠ¤í‚¤ë§ˆ(userId, movieId, rating, timestamp)ë¡œ ë³€í™˜:
      A) netflix ì›ë³¸:   movieId, customerId, rating, date
      B) ìš°ë¦¬ê°€ ë§Œë“  ìƒ˜í”Œ: userId, movieId, rating, timestamp
      C) í˜¼í•©í˜•:         userId, movieId, rating, date  (dateë§Œ ìˆì„ ë•Œ)
    ë°˜í™˜: DataFrame[userId(int64), movieId(int64), rating(float/Int), timestamp(datetime64[ns])]
    """
    df = pd.read_parquet(path)
    cols = set(df.columns)

    if {"movieId","customerId","rating","date"}.issubset(cols):
        d = df.rename(columns={"customerId":"userId"}).copy()
        d["timestamp"] = pd.to_datetime(d["date"], errors="coerce")
        d = d.dropna(subset=["timestamp"])
        d = d[["userId","movieId","rating","timestamp"]]
    elif {"userId","movieId","rating","timestamp"}.issubset(cols):
        d = df.copy()
        d["timestamp"] = pd.to_datetime(d["timestamp"], errors="coerce")
        d = d.dropna(subset=["timestamp"])
        d = d[["userId","movieId","rating","timestamp"]]
    elif {"userId","movieId","rating","date"}.issubset(cols):
        d = df.copy()
        d["timestamp"] = pd.to_datetime(d["date"], errors="coerce")
        d = d.dropna(subset=["timestamp"])
        d = d[["userId","movieId","rating","timestamp"]]
    else:
        raise ValueError(
            f"[interactions] unsupported schema: found {sorted(df.columns)}; "
            "expected one of: "
            "{movieId,customerId,rating,date} or "
            "{userId,movieId,rating,timestamp} or "
            "{userId,movieId,rating,date}"
        )

    # íƒ€ì… ì •ë¦¬
    d = d.astype({"userId":"int64","movieId":"int64"})
    return d

# ì¶”ì²œ ì§€í‘œ í•¨ìˆ˜(Precision, Recall, NDCG, MAP, HitRate)
def _to_float_array(x):
    # NumPy 2.x í˜¸í™˜: asfarray ëŒ€ì‹  asarray(dtype=float)
    return np.asarray(x, dtype=float)

def dcg_at_k(rels, k):
    rels = _to_float_array(rels)[:k]
    if rels.size:
        return np.sum((2.0**rels - 1.0) / np.log2(np.arange(2, rels.size + 2)))
    return 0.0

def ndcg_at_k(binary_hits, k):
    arr = _to_float_array(binary_hits)
    dcg = dcg_at_k(arr, k)
    ideal = dcg_at_k(sorted(arr, reverse=True), k)
    return dcg / ideal if ideal > 0 else 0.0

def precision_at_k(binary_hits, k):
    arr = _to_float_array(binary_hits)[:k]
    return float(arr.mean()) if arr.size else 0.0

def recall_at_k(binary_hits, k, num_positives):
    arr = _to_float_array(binary_hits)[:k]
    return float(arr.sum()) / max(1, int(num_positives))

def average_precision_at_k(binary_hits, k):
    arr = _to_float_array(binary_hits)[:k]
    hits = 0.0
    s = 0.0
    for i, h in enumerate(arr, start=1):
        if h > 0:
            hits += 1.0
            s += hits / i
    return s / max(1.0, arr.sum())

def hit_rate_at_k(binary_hits, k):
    arr = _to_float_array(binary_hits)[:k]
    return 1.0 if np.any(arr > 0) else 0.0

# ì¶”ì²œ ê²°ê³¼ì— ì œëª©/ì—°ë„ ë§¤í•‘
def build_movie_meta_map(content_df, tmdb_df):
    meta = (content_df[['movieId','title']]
            .merge(tmdb_df[['movieId','title_tmdb','release_year','original_language']],
                   on='movieId', how='left'))
    def _title(row):
        return row['title_tmdb'] if isinstance(row['title_tmdb'], str) and len(row['title_tmdb'])>0 else row['title']
    def _year(row):
        try:
            return int(str(row['release_year'])[:4]) if row['release_year'] else None
        except Exception:
            return None
    meta['rec_title'] = meta.apply(_title, axis=1)
    meta['rec_year']  = meta.apply(_year, axis=1)
    mid2title = dict(zip(meta['movieId'], meta['rec_title']))
    mid2year  = dict(zip(meta['movieId'], meta['rec_year']))
    mid2lang  = dict(zip(meta['movieId'], meta['original_language'].astype(str)))
    return mid2title, mid2year, mid2lang

from sklearn.preprocessing import normalize as sk_normalize
import csv
# predict with test dataset(netflix_ratings)
def evaluate_cbf_offline_netflix(
    CONTENT_PATH, TMDB_PATH, GENOME_PATH, RATINGS_PATH,
    pos_threshold=4.0, leave_last_k=1, min_train_pos=3,
    ks=(10, 20, 50),
    weights=(0.6, 0.8, 2.0),
    topk_tags_per_movie=75,
    filter_lang=None,
    topn_eval=20000,
    use_parquet_loaders=True,
    export_recs_path="cbf_recommend_movie.csv",
    recs_topk=50,             # ìœ ì €ë³„ ì €ì¥ ê°œìˆ˜
    export_scores=True,       # ì ìˆ˜ ì»¬ëŸ¼ í¬í•¨ì—¬ë¶€
    debug=True,              # ë””ë²„ê·¸ on/off
    debug_users=3,            # ë””ë²„ê·¸ë¡œ ê¹Šê²Œ ë³¼ ìœ ì € ìˆ˜
    debug_topn=20000          # ë””ë²„ê·¸ ìƒìœ„ ë­í¬ ì»·
):
    # 1) load datas
    content = load_movielens_content(CONTENT_PATH)
    tmdb    = load_tmdb_items(TMDB_PATH)
    genome  = load_genome_long(GENOME_PATH)
    ratings = load_interactions_auto(RATINGS_PATH)  # userId,movieId,rating,timestamp

    # ì„ í˜¸ ì–¸ì–´ ë°ì´í„° ìˆì„ ì‹œ í›„ë³´êµ° ì œí•œì— ë°˜ì˜
    if filter_lang:
        meta = content.merge(tmdb, on="movieId", how="left")
        ok_lang = meta["original_language"].astype(str).str.lower().isin([s.lower() for s in filter_lang])
        keep_ids = set(meta.loc[ok_lang, "movieId"].tolist())
        ratings = ratings[ratings["movieId"].isin(keep_ids)]
        content = content[content["movieId"].isin(keep_ids)]
        tmdb    = tmdb[tmdb["movieId"].isin(keep_ids)]
        genome  = genome[genome["movieId"].isin(keep_ids)]

    # 2) item feature vector ê¸°ë°˜ matrices
    X_items, movie_index, vocab = build_item_feature_matrices(
        content, tmdb, genome, weights=weights, topk_tags_per_movie=topk_tags_per_movie
    )

    N = X_items.shape[0]
    pos_map = {mid: i for i, mid in enumerate(movie_index)}
    mid2title, mid2year, mid2lang = build_movie_meta_map(content, tmdb)

    """
    ë””ë²„ê·¸ ì¶”ê°€1
    if debug:
    # í–‰/ì—´ ë…¸ë¦„ í†µê³„ë¡œ ì •ê·œí™” ìƒíƒœ ì ê²€
        def _row_norm_stats(X):
            rn = np.sqrt(np.asarray((X.multiply(X)).sum(axis=1)).ravel())
            print("[DEBUG/X] row-norm mean/median/min/max:",
                float(rn.mean()), float(np.median(rn)), float(rn.min()), float(rn.max()))
        _row_norm_stats(X_items)
    """

    # 3) Train/Test split (LLkO)
    train_pos, test_pos, seen = temporal_leave_last_k_out(
        ratings, k=leave_last_k, pos_threshold=pos_threshold
    )

    # Popularity ë¸”ë Œë”© - Trainë§Œ ì¶”ì¶œ (ìœ ì €ë³„ ë§ˆì§€ë§‰ kê°œ ì œì™¸)
    ratings_sorted_pop = ratings.sort_values("timestamp")
    train_only = (ratings_sorted_pop
        .groupby("userId", sort=False, group_keys=False)
        .apply(lambda g: g.iloc[:-leave_last_k] if len(g) > leave_last_k else g.iloc[0:0],
                include_groups=False)
        .reset_index(drop=True))
    # movieId â†’ ë“±ì¥ ë¹ˆë„ â†’ ë¡œê·¸ ìŠ¤ì¼€ì¼ â†’ 0~1 ì •ê·œí™”
    item_pop = train_only["movieId"].value_counts().to_dict()
    pop_vec = np.array([np.log1p(item_pop.get(mid, 0)) for mid in movie_index], dtype=float)
    if pop_vec.max() > 0:
        pop_vec = pop_vec / pop_vec.max()
    else:
        pop_vec = np.zeros_like(pop_vec)

    # movieId -> ì¥ë¥´ ë¦¬ìŠ¤íŠ¸ ë§µ (ì¥ë¥´ê°€ listê°€ ì•„ë‹ˆë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸)
    mid2genres = dict(zip(
        content["movieId"],
        content["genres_parsed"].apply(lambda x: x if isinstance(x, list) else [])
    ))
    # movieId -> ê°ë… ë§µ
    mid2dirs = dict(zip(
        tmdb["movieId"],
        tmdb["director_list"].apply(lambda x: x if isinstance(x, list) else [])
    ))
    from collections import Counter
    # ìœ ì € ìƒìœ„ ì¥ë¥´ ì¶”ì¶œ (train set ê¸°ì¤€)
    def _top_user_genres(train_items, topn=3):
        c = Counter()
        for m in train_items:
            for g in mid2genres.get(m, []):
                c[g] += 1
        # ìƒìœ„ topn ì¥ë¥´ì˜ ì§‘í•© ë°˜í™˜
        return {g for g, _ in c.most_common(topn)}
    # ìœ ì € ìƒìœ„ ê°ë… ì¶”ì¶œ (train set ê¸°ì¤€)
    def _top_user_directors(train_items, topn=3):
        c = Counter()
        for m in train_items:
            for d in mid2dirs.get(m, []):
                c[d] += 1
        return {d for d,_ in c.most_common(topn)}

    # train_pos: {userId -> set(movieId)}
    ratings_sorted = ratings.sort_values(["userId","timestamp","movieId"])
    train_pos_ratings = {}  # {uid: {movieId: rating}}
    train_neg_items   = {}  # {uid: set(movieId)}

    for uid, g in ratings_sorted.groupby("userId", sort=False):
        if len(g) <= leave_last_k:
            continue
        g_train = g.iloc[:-leave_last_k]
        pos_dict, neg_set = {}, set()
        for m, r in zip(g_train["movieId"].values, g_train["rating"].values):
            if r >= 4.0:
                pos_dict[m] = float(r)
            elif r <= 2.0:
                neg_set.add(m)
        if pos_dict:
            train_pos_ratings[uid] = pos_dict
        if neg_set:
            train_neg_items[uid] = neg_set
    """
    # ë””ë²„ê·¸ ì¶”ê°€2
    if debug:
        # ë¹ ë¥¸ HitRate@50ë¥¼ 500ëª…ì˜ ìœ ì €ì— ëŒ€í•´ ìƒ˜í”Œ ì¸¡ì •
        def _quick_hitrate_at_k(train_pos, test_pos, seen, X_items, movie_index, K=50, max_users=500):
            pos_map = {mid:i for i, mid in enumerate(movie_index)}
            users = list(sorted(set(train_pos) & set(test_pos)))[:max_users]
            hits = total = 0
            for _uid in users:
                tr, te = train_pos[_uid], test_pos[_uid]
                if not tr or not te:
                    continue
                rows = [pos_map[m] for m in tr if m in pos_map]
                U_sum = X_items[rows].sum(axis=0)
                U_arr = np.asarray(U_sum).ravel()
                if U_arr.sum() == 0:  # ìœ ì € ë²¡í„°ê°€ 0ì¸ ê²½ìš° pass
                    continue
                U_arr = U_arr / float(len(rows))
                u = sk_normalize(sparse.csr_matrix(U_arr), norm="l2").toarray().ravel()

                ban = {pos_map[m] for m in seen[_uid] if m in pos_map}
                te_idx = {pos_map[m] for m in te if m in pos_map}
                ban -= te_idx
                mask = np.ones(X_items.shape[0], dtype=bool)
                if ban:
                    mask[np.fromiter(ban, dtype=int)] = False

                sims = X_items.dot(u)
                sims[~mask] = -1.0
                k = min(K, X_items.shape[0])
                idx = np.argpartition(-sims, k-1)[:k]
                if any(i in te_idx for i in idx):
                    hits += 1
                total += 1
            print(f"[DEBUG] Quick HitRate@{K}: {hits}/{total} = {hits/max(1,total):.3%}")

        _quick_hitrate_at_k(train_pos, test_pos, seen, X_items, movie_index, K=50, max_users=500)

    def inspect_X_items(X_items, movie_index):
        N, D = X_items.shape
        nnz = X_items.nnz
        density = nnz / (N * D)
        row_nnz = np.diff(X_items.indptr)  # ê° rowì˜ nnz
        zero_rows = int(np.sum(row_nnz == 0))
        print(f"[X_items] shape={X_items.shape}, nnz={nnz:,}, density={density:.6f}")
        print(f"[X_items] zero-feature rows: {zero_rows}/{N} = {zero_rows/max(1,N):.3%}")
        return row_nnz

    row_nnz = inspect_X_items(X_items, movie_index)

    # ì»¤ë²„ë¦¬ì§€/ëˆ„ìˆ˜ ì§„ë‹¨ì€ ë£¨í”„ ë°–ì—ì„œ í•œ ë²ˆë§Œ
    test_all = [m for s in test_pos.values() for m in s]
    test_in_items = [m for m in test_all if m in pos_map]
    cov_items = len(test_in_items) / max(1, len(test_all))
    overlaps = 0
    tot = 0
    for _uid in set(train_pos) & set(test_pos):
        te = test_pos[_uid]
        sv = seen[_uid]
        tot += len(te)
        overlaps += sum(1 for m in te if m in sv)
    print(f"[COVERAGE] test items in X_items: {len(test_in_items)}/{len(test_all)} = {cov_items:.3%}")
    print(f"[LEAKAGE]  testâˆ©seen count: {overlaps}/{tot} = {overlaps/max(1,tot):.3%}")
    """

    def score_dot(U_vec, X_items_mat):
        return (U_vec @ X_items_mat.T).toarray().ravel()  # 1xN ndarray

    # ì¶”ì²œ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë° ì €ì¥
    stream_save = export_recs_path is not None
    if stream_save:
        header = [
            "userId","rank","movieId","title","year","language",
            "score_final","score_blend","score_cbf","bonus",
            "is_test_item","was_seen_in_train"
        ] if export_scores else [
            "userId","rank","movieId","title","year","language","score_final",
            "is_test_item","was_seen_in_train"
        ]
        # ìƒˆë¡œ ì‹œì‘
        with open(export_recs_path, "w", newline="", encoding="utf-8") as f:
            w = csv.writer(f)
            w.writerow(header)

    # 4) user ë³„ predict ì§„í–‰
    results = []
    users = sorted(set(train_pos.keys()) & set(test_pos.keys()))
    printed = 0
    """
        Leave-Last-k-Outìœ¼ë¡œ ì¸í•œ user trainì—ì„œ
        íŠ¹ì§• í‰ê·  ë²¡í„° ìƒì„±
        Trainì—ì„œ ì¢‹ì•„ìš”(ì–‘ì„±)ì¸ ì•„ì´í…œë“¤ì˜ íŠ¹ì§•ì„ í‰ê· í•´ ìœ ì € ë²¡í„° ìƒì„±.
        - np.matrix ì´ìŠˆ ë°©ì§€: sumâ†’ndarrayâ†’CSRâ†’normalize
    """
    # 1. í‰ì  4.0 ì´ìƒ ì˜í™”ì— ëŒ€í•œ ìœ ì € ë²¡í„° ìƒì„±
    def build_user_vector_by_training_items(user_train_positive_items, pos_map, X_items):
        if not user_train_positive_items:
            return None
        rows = [pos_map[m] for m in user_train_positive_items if m in pos_map]
        if not rows:
            return None
        U_sum = X_items[rows].sum(axis=0)
        U_arr = np.asarray(U_sum).ravel()
        if not np.any(U_arr):
            return None
        U_arr = U_arr / float(len(rows))
        U = sparse.csr_matrix(U_arr)
        return sk_normalize(U, norm="l2")
    # 2. í‰ì (ì˜ˆ: 4~5)ì„ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©í•˜ì—¬ ìœ ì € í”„ë¡œíŒŒì¼ ìƒì„±.
    def build_user_vector_weighted(user_train_ratings_dict, pos_map, X_items, min_pos=1):
        """
        user_train_ratings_dict: {movieId -> rating}  (Train ë²”ìœ„ì˜ í‰ì )
        """
        if not user_train_ratings_dict:
            return None
        rows, ws = [], []
        for m, r in user_train_ratings_dict.items():
            if m in pos_map and r >= 4.0:   # positiveë§Œ ì“°ë˜, í‰ì ì„ ê°€ì¤‘ì¹˜ë¡œ
                rows.append(pos_map[m])
                ws.append(float(r))         # 4.0/4.5/5.0 ë“± ì°¨ì´ë¥¼ ë°˜ì˜
        if len(rows) < min_pos:
            return None

        # ê°€ì¤‘ í•© â†’ í‰ê·  â†’ CSR â†’ L2
        W = np.asarray(ws, dtype=float)
        # X_items[rows] (len(rows) x D) ì— í–‰ë³„ ê°€ì¤‘ì¹˜ ì ìš©: (W[:,None]) ë¡œ ë¸Œë¡œë“œìºìŠ¤íŠ¸
        U_sum = X_items[rows].multiply(W[:, None]).sum(axis=0)
        U_arr = np.asarray(U_sum).ravel() / W.sum()
        if not np.any(U_arr):
            return None
        U = sparse.csr_matrix(U_arr)
        return sk_normalize(U, norm="l2")
    # 3. ì„ í˜¸ë„ ë‚®ì€ ì¥ë¥´ ìœ ì € ë²¡í„° ìƒì„±
    def build_user_vector_rocchio(pos_ratings_dict, neg_items_set, pos_map, X_items,
                              alpha=1.0, beta=0.3, min_pos=2):
        if not pos_ratings_dict or len(pos_ratings_dict) < min_pos:
            return None

        # positive: í‰ì  ê°€ì¤‘ í‰ê· 
        pr, pw = [], []
        for m, r in pos_ratings_dict.items():
            if m in pos_map:
                pr.append(pos_map[m]); pw.append(float(r))
        if not pr:
            return None
        pw = np.asarray(pw, dtype=float)
        P = X_items[pr].multiply(pw[:, None]).sum(axis=0)
        P = np.asarray(P).ravel() / pw.sum()

        # negative: ë‹¨ìˆœ í‰ê· (ìˆìœ¼ë©´)
        N = None
        if neg_items_set:
            nr = [pos_map[m] for m in neg_items_set if m in pos_map]
            if nr:
                N_sum = X_items[nr].sum(axis=0)
                N = np.asarray(N_sum).ravel() / float(len(nr))

        U_arr = alpha * P - (beta * N if N is not None else 0.0)
        if not np.any(U_arr):
            return None
        U = sparse.csr_matrix(U_arr)
        return sk_normalize(U, norm="l2")

    # í•˜ì´í¼íŒŒë¼ë¯¸í„°ğŸ”
    blend_alpha = 0.70
    topm_ratio  = 0.15
    bonus_p     = 72
    bonus_dir   = 0.03
    bonus_genre = 0.015

    for uid in users:
        train_set = train_pos[uid]
        test_set  = test_pos[uid]
        if len(train_set) < min_train_pos or len(test_set) == 0:
            continue

        pos_dict = train_pos_ratings.get(uid)
        neg_set  = train_neg_items.get(uid, set())
        U = build_user_vector_rocchio(pos_dict, neg_set, pos_map, X_items, alpha=1.0, beta=0.3, min_pos=2)
        if U is None:
            # fallback: í‰ì  ê°€ì¤‘ or í‰ê·  ì»·
            U = build_user_vector_weighted(pos_dict, pos_map, X_items, min_pos=2)
            # U = build_user_vector_by_training_items(train_set, movie_index, X_items)
        if U is None:
            continue

        # í›„ë³´ ë§ˆìŠ¤í¬: seen ì œì™¸í•˜ë˜, test ì•„ì´í…œì€ ë‹¤ì‹œ í—ˆìš©(ëˆ„ìˆ˜ ë°©ì§€)
        ban = set(pos_map[m] for m in seen[uid] if m in pos_map)
        test_idx = {pos_map[m] for m in test_set if m in pos_map}
        ban -= test_idx  # test item ì œì™¸í•˜ì§€ ì•ŠìŒ

        mask = np.ones(N, dtype=bool)
        if ban:
            mask[np.fromiter(ban, dtype=int)] = False

        # ===== ì¥ë¥´/ê°ë… í”„ë¦¬í•„í„° =====
        # ìœ ì € ìƒìœ„ ì¥ë¥´ ì¶”ì¶œ (train_set ì‚¬ìš©)
        user_top_gen = _top_user_genres(train_set, topn=3)
        # ìœ ì € ìƒìœ„ ê°ë… ì¶”ì¶œ
        top_dirs = _top_user_directors(train_set, topn=3)

        if user_top_gen:
            # ê° movie_indexë³„ ìœ ì € ìƒìœ„ ì¥ë¥´ì™€ 1ê°œë¼ë„ ê²¹ì¹˜ë©´ True
            mask_gen = np.fromiter(
                (
                    any(g in user_top_gen for g in mid2genres.get(mid, []))
                    for mid in movie_index
                ),
                dtype=bool,
                count=N
            )
            # í›„ë³´ê°€ ë„ˆë¬´ ì¤„ì–´ë“¤ë©´(3000ê°œ ì´í•˜) í”„ë¦¬í•„í„° ì™„í™”í•´ ë¦¬ì½œ ë³´ì¡´
            # ì„ê³„ì¹˜ëŠ” ë°ì´í„° í¬ê¸°ì— ë§ì¶° ì¡°ì • (ì˜ˆ: max(3000, int(0.05*N)))
            GEN_MIN_CAND = max(3000, int(0.05 * N))  # ì™„í™” ì„ê³„ì¹˜
            if mask_gen.sum() > GEN_MIN_CAND:
                mask &= mask_gen
                if debug and printed < debug_users:
                    print(f"[DEBUG/user {uid}] genre-prefilter kept {int(mask_gen.sum())}/{N} candidates")
            else:
                if debug and printed < debug_users:
                    print(f"[DEBUG/user {uid}] genre-prefilter skipped (kept={int(mask_gen.sum())} < {GEN_MIN_CAND})")

            # ì¥ë¥´ í”„ë¦¬í•„í„° ì´í›„ì—ë„ test ì•„ì´í…œì€ ë°˜ë“œì‹œ í›„ë³´ë¡œ ë˜ì‚´ë¦°ë‹¤
            if test_idx:
                mask[np.fromiter(test_idx, dtype=int, count=len(test_idx))] = True

        # ìœ ì‚¬ë„ -> ìƒìœ„ Topn_eval ì¶”ì¶œ
        u_arr = U.toarray().ravel()  # (D,)
        # 1) ìˆœìˆ˜ CBF
        sims_cbf = score_dot(U, X_items)
        sims_cbf[~mask] = -1.0

        # 2) ì¸ê¸°ë„ ë¸”ë Œë”©(sims_cbf, pop_vec ìŠ¤ì¼€ì¼ í‘œì¤€í™”)
        from sklearn.preprocessing import StandardScaler
        _scaler1 = StandardScaler(with_mean=True, with_std=True)
        _scaler2 = StandardScaler(with_mean=True, with_std=True)

        cbf_std  = _scaler1.fit_transform(sims_cbf.reshape(-1,1)).ravel()
        pop_std  = _scaler2.fit_transform(pop_vec.reshape(-1,1)).ravel()

        sims_blend = blend_alpha * cbf_std + (1 - blend_alpha) * pop_std
        sims_blend[~mask] = -1.0

        # 3) ìƒìœ„ í›„ë³´ í•œì • ë³´ë„ˆìŠ¤ ê°€ì  (ê°ë…/ì¥ë¥´)
        mask_true = int(mask.sum())
        M = max(1000, min(5000, int(topm_ratio * mask_true)))   # í›„ë³´ì˜ ì•½ 15% (1k~5k ì‚¬ì´ í´ë¨í•‘)
        # ìƒìœ„ M í›„ë³´(ë¸”ë Œë”© ì ìˆ˜ ê¸°ì¤€) ì¶”ì¶œ
        topM_idx = np.argpartition(-sims_blend, M-1)[:M]
        # í…ŒìŠ¤íŠ¸ ì•„ì´í…œì´ TopMì— ë°˜ë“œì‹œ í¬í•¨ë˜ë„ë¡
        if test_idx:
            tj = next(iter(test_idx))  # í•˜ë‚˜ ëŒ€í‘œë¡œ
            if tj not in topM_idx:
                topM_idx[-1] = tj

        # TopM ì•ˆì—ì„œë„ ìƒìœ„ p-íƒ€ì¼ë§Œ ë³´ë„ˆìŠ¤ ì ìš©
        th = np.percentile(sims_blend[topM_idx], bonus_p)
        eligible = np.zeros(N, dtype=bool)
        eligible[topM_idx] = sims_blend[topM_idx] >= th

        bonus = np.zeros(N, dtype=float)
        if top_dirs:
            has_top_dir = np.fromiter(
                (any(d in top_dirs for d in mid2dirs.get(mid, [])) for mid in movie_index),
                dtype=bool, count=N
            )
            bonus[has_top_dir & eligible] += bonus_dir
        if user_top_gen:
            has_top_gen = np.fromiter(
                (any(g in user_top_gen for g in mid2genres.get(mid, [])) for mid in movie_index),
                dtype=bool, count=N
            )
            bonus[has_top_gen & eligible] += bonus_genre
        bonus[~mask] = 0.0  # ë§ˆìŠ¤í¬ ë°”ê¹¥ì€ ë¬´íš¨

        # === 2ë‹¨ê³„ ì •ë ¬: 1) sims_blend DESC, 2) bonus DESC ===
        tm = topM_idx
        # lexsortëŠ” (-sims_blend)ê°€ 1ì°¨, (-bonus)ê°€ 2ì°¨ê°€ ë˜ë„ë¡
        order = np.lexsort((-bonus[tm], -sims_blend[tm]))
        topM_sorted = tm[order]
        # ë‚˜ë¨¸ì§€ í›„ë³´ëŠ” ë¸”ë Œë”© ì ìˆ˜ ë‹¨ì¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        rest_mask = mask.copy()
        rest_mask[topM_sorted] = False
        rest_idx = np.where(rest_mask)[0]
        rest_order = rest_idx[np.argsort(-sims_blend[rest_idx])]
        # ìµœì¢… ë­í‚¹ ì¸ë±ìŠ¤
        idx = np.concatenate([topM_sorted, rest_order])

        # topn_evalë¡œ ì»·
        K = min(int(topn_eval), N)
        if K < idx.size:
            idx = idx[:K]

        # ìœ ì €ë³„ ì¶”ì²œê²°ê³¼ ì €ì¥
        if stream_save:
            Ksave = min(recs_topk, idx.size)
            out_rows = []
            seen_set = set(seen[uid])
            for rank, j in enumerate(idx[:Ksave], start=1):
                mid = movie_index[j]
                row_base = [
                    uid, rank, int(mid),
                    mid2title.get(mid, None),
                    mid2year.get(mid, None),
                    mid2lang.get(mid, None),
                ]
                if export_scores:
                    row = row_base + [
                        float(sims_blend[j] + bonus[j]),  # score_final
                        float(sims_blend[j]),
                        float(sims_cbf[j]),
                        float(bonus[j]),
                        int(j in test_idx),         # is_test_item
                        int(mid in seen_set)        # was_seen_in_train
                    ]
                else:
                    row = row_base + [
                        float(sims_blend[j] + bonus[j]),
                        int(j in test_idx),
                        int(mid in seen_set)
                    ]
                out_rows.append(row)

            # append
            with open(export_recs_path, "a", newline="", encoding="utf-8") as f:
                w = csv.writer(f); w.writerows(out_rows)

        # === ë””ë²„ê·¸ ì¶œë ¥ (ìƒìœ„ debug_usersë§Œ) ===
        if debug and printed < debug_users:
            # í…ŒìŠ¤íŠ¸ ì•„ì´í…œ ê°ê°ì˜ (CBF â†’ BLEND â†’ FINAL) ì ìˆ˜ì™€ ë­í¬ ë³€í™” í™•ì¸
            test_idx_list = sorted(test_idx)
            for j in test_idx_list:
                score_cbf   = float(sims_cbf[j])
                score_blend = float(sims_blend[j])
                # ìµœì¢… ì •ë ¬ì€ ì‚¬ì „ì‹(-sims_blend, -bonus)
                score_bonus = float(bonus[j])

                # ë­í¬: ë‚˜ë³´ë‹¤ í° ì ìˆ˜ ê°¯ìˆ˜ + 1
                rank_cbf   = int((sims_cbf   > sims_cbf[j]).sum() + 1)
                rank_blend = int((sims_blend > sims_blend[j]).sum() + 1)

                # FINAL ë­í¬ëŠ” idx(ìµœì¢… ì •ë ¬ ê²°ê³¼)ì—ì„œì˜ ìœ„ì¹˜
                try:
                    rank_final = int(np.where(idx == j)[0][0]) + 1
                except Exception:
                    rank_final = None  # ìµœì¢… idx ì»· ë°–ì¸ ê²½ìš°
                print(
                    f"[DEBUG/user {uid}] test-item {movie_index[j]} | "
                    f"rank CBFâ†’BLENDâ†’FINAL: {rank_cbf} â†’ {rank_blend} â†’ {rank_final} | "
                    f"scores: CBF={score_cbf:.5f}, BLEND={score_blend:.5f}, BONUS={score_bonus:.5f}"
                )

            # ë¸”ë Œë”©/ë³´ë„ˆìŠ¤ê°€ í›„ë³´ ì „ì²´ì— ì–¼ë§ˆë‚˜ ì˜í–¥ì„ ì¤¬ëŠ”ì§€ ìš”ì•½
            diff_blend = sims_blend - sims_cbf
            diff_bonus = bonus
            # ë§ˆìŠ¤í¬ëœ(-1) ì œì™¸í•˜ê³  ë¶„í¬ë§Œ
            valid = mask
            def _pct(a, q):
                return float(np.percentile(a[valid], q)) if valid.sum() else 0.0
            print(
                f"[DEBUG/user {uid}] Î”blend (p50/p90/max): "
                f"{_pct(diff_blend,50):.5f}/{_pct(diff_blend,90):.5f}/{float(diff_blend[valid].max() if valid.sum() else 0.0):.5f}"
            )
            print(
                f"[DEBUG/user {uid}] bonus  (p50/p90/max): "
                f"{_pct(diff_bonus,50):.5f}/{_pct(diff_bonus,90):.5f}/{float(diff_bonus[valid].max() if valid.sum() else 0.0):.5f}"
            )
            # ë³´ë„ˆìŠ¤ê°€ ì‹¤ì œë¡œ ì ìš©ëœ í›„ë³´ ìˆ˜ (TopMì—ë§Œ ë¶€ì—¬)
            print(f"[DEBUG/user {uid}] TopM={len(topM_idx)}  bonus>0 in TopM={int((bonus[topM_idx] > 0).sum())}")
            # í…ŒìŠ¤íŠ¸ ì•„ì´í…œì´ TopMì— í¬í•¨ë˜ì—ˆëŠ”ì§€ / í…ŒìŠ¤íŠ¸ ë³´ë„ˆìŠ¤ ê°’
            in_topM = any(j in set(topM_idx) for j in test_idx_list)
            test_bonus_vals = [float(bonus[j]) for j in test_idx_list] if test_idx_list else []
            print(
                f"[DEBUG/user {uid}] test_in_TopM={in_topM}  "
                f"test_bonus_vals={','.join(f'{v:.5f}' for v in test_bonus_vals)}  alpha={blend_alpha}"
            )
            printed += 1

        # hits ê³„ì‚°
        hits = [1 if i in test_idx else 0 for i in idx[:max(ks)]]

        row = {
            "userId": uid,
            "num_train_pos": len(train_set),
            "num_test_pos": len(test_set)
        }
        for k in ks:
            row[f"Precision@{k}"] = precision_at_k(hits, k)
            row[f"Recall@{k}"]    = recall_at_k(hits, k, num_positives=len(test_set))
            row[f"NDCG@{k}"]      = ndcg_at_k(hits, k)
            row[f"MAP@{k}"]       = average_precision_at_k(hits, k)
            row[f"HitRate@{k}"]   = hit_rate_at_k(hits, k)
        results.append(row)

    per_user = pd.DataFrame(results)
    if len(per_user)==0:
        print("No eligible users. Adjust pos_threshold/min_train_pos/leave_last_k or check ID mapping.")
        return None, None

    summary = per_user[[c for c in per_user.columns if "@" in c]].mean().to_frame("mean").T
    return per_user, summary

per_user, summary = evaluate_cbf_offline_netflix(
    CONTENT_PATH, TMDB_PATH, GENOME_PATH, SAMPLED_RATINGS_PATH,
    pos_threshold=4.0,      # í‰ì  4.0 ì´ìƒë§Œ ê³ ë ¤
    leave_last_k=1,         # ìœ ì €ë³„ ë§ˆì§€ë§‰ 1ê°œë¥¼ Test
    min_train_pos=3,        # Train positive rate ìµœì†Œ 3ê°œ ì´ìƒ
    ks=(10,20,50),
    weights=(0.6, 0.8, 2.0),
    topk_tags_per_movie=75,
    export_recs_path="cbf_recs_test.csv",
    recs_topk=100,
    export_scores=True,
    filter_lang=None        # ["en","ko"]
)

if per_user is not None:
    print(summary)
    per_user.to_csv("cbf_eval_netflix_per_user.csv", index=False, encoding="utf-8-sig")
    summary.to_csv("cbf_eval_netflix_summary.csv",  index=False, encoding="utf-8-sig")
    print("Saved per-user & summary CSV.")

"""By CDF(Cumulative Distribution Function, Percent Point Function), Transform score_final(cbf_score)"""

import pandas as pd
import numpy as np
df = pd.read_csv("cbf_recs_test.csv")

df["rank"] = df.groupby("userId")["score_final"].rank(method="average")
n = df.groupby("userId")["score_final"].transform("count")
df["prob_ecdf"] = (df["rank"] - 1) / (n - 1 + 1e-9)

df.to_csv("cbf_recs_ecdf.csv", index=False)